> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [mp.weixin.qq.com](https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&mid=2247509373&idx=1&sn=8686bfdc5fa52ada7f9bb271af0833b7&chksm=fb3a0076cc4d896088bc6dafeb584236cb7680a5ac6ca687d2d0602e68b70c938727a20f74eb&mpshare=1&scene=1&srcid=0605nzfkfZafMWacMbhP6WCx&sharer_sharetime=1622870257758&sharer_shareid=7fece245937ac96f04f0fb8e1311fff1#rd)

### **1. 监督式学习：**

![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qzfSdHsQCQnFVun6SiakCic9qdQ2P6XfrMBdGmc4FXnYNUedsKNybgs5g/640?wx_fmt=jpeg)

在监督式学习下，输入数据被称为 “训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据” 的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。**监督式学习的常见应用场景如分类问题和回归问题。**常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）

### **2. 非监督式学习：**

![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qOulZcKQOD2gJtTuiaM9LD5Jo7cdxaAdsAjdfic4EHzpzqPH9YA9Cps1Q/640?wx_fmt=jpeg)

在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。**常见的应用场景包括关联规则的学习以及聚类等。**常见算法包括 Apriori 算法以及 k-Means 算法。

### **3. 半监督式学习：**

![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qdwOiawS0uGnwpMgtOJJ8d3zO3xHKdCfAibicVHfdjYYv9gEYmzOFJBRLA/640?wx_fmt=png)

在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如**图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。**

### **4. 强化学习：**

![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qYVqLterKGPxPnPvLNPcJJq3xnrUgiaxbyBsodppLD2qCoUR0xwiaCfeQ/640?wx_fmt=jpeg)

在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。**常见算法包括 Q-Learning 以及时间差学习（Temporal difference learning）**

在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。

**5. 算法类似性**
------------

根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，**同一分类的算法可以针对不同类型的问题。**这里，我们尽量把常用的算法按照最容易理解的方式进行分类。

### **6. 回归算法：**

![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qOpKRUKLExQ7pPxPeHswdAt9ZvTYiajC26L2QbpibctibicOojhib76lZr7w/640?wx_fmt=jpeg)

回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。**回归算法是统计机器学习的利器。**在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）

### **7. 基于实例的算法**

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qTjsavVGric789lHAVuVia7fgxibHvF3Q85aQwaj9ibvibV541IPcLfJtCRg/640?wx_fmt=jpeg) 

基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为 “赢家通吃” 学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map ， SOM）

### **8. 正则化方法**

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qclL4ua7d0pav3z8Xkz6bAZrxsmiazeJBMGQbc0gjRZ59HNbI9b8IScA/640?wx_fmt=png) 

正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。**正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。**常见的算法包括：Ridge Regression，Least Absolute Shrinkage and Selection Operator（LASSO），以及弹性网络（Elastic Net）。

### **9. 决策树学习**

![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qrmbkaicyu3kyt8gXickEGY3o5rTVrGRA7jBF6C9MvTZCSdzT8Q2mnuSA/640?wx_fmt=png)

决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）， ID3 (Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM）

### **10. 贝叶斯方法**

![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qibjvLtbBPYk7e0Mde3qaoCQhZbYfQ01c4J1bic9kg8VyYKmZyAU7unBA/640?wx_fmt=jpeg)

贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators， AODE），以及 Bayesian Belief Network（BBN）。

### **11. 基于核的算法**

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q2rUL0fPdaicA3tb2vib81C55dXlsFRLsbeFhavhyTQ19iabUqdicoApuOw/640?wx_fmt=jpeg) 

基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。常见的基于核的算法包括：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 以及线性判别分析（Linear Discriminate Analysis ，LDA) 等

### **12. 聚类算法**

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qcvoZq01dJseRbO39kc3HuyA2Osn7iaeUf2GicA7favDasV3jF1F6DgnQ/640?wx_fmt=png) 

聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。**所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。**常见的聚类算法包括 k-Means 算法以及期望最大化算法（Expectation Maximization， EM）。

### **13. 关联规则学习**

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qJ5ZDDiaWHGWaDx5VL2cdf7KQOZrlTwzz2LQovJX1icmj5DomwmGbRkUg/640?wx_fmt=jpeg) 

关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori 算法和 Eclat 算法等。

### **14. 人工神经网络**

![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qicV3tzk3wSo826wjicxupzf6awyaQQ81ZeWicsJw0KL44SNic6gQVZguEg/640?wx_fmt=jpeg) 

人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield 网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ）

### **15. 深度学习**

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qvSicnrwQGfdHrRW0UuXIw4BaON9PunmWsyCeYG0FyCj9pEGDiavDEpicQ/640?wx_fmt=jpeg) 

深度学习算法是对人工神经网络的发展。在近期赢得了很多关注， 特别是百度也开始发力深度学习后， 更是在国内引起了很多关注。   在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）， Deep Belief Networks（DBN），卷积网络（Convolutional Network）, 堆栈式自动编码器（Stacked Auto-encoders）。

### **16. 降低维度算法**

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qwcneLRrjKrQrC5OBqVKLeouEvDB8ibV7UNCpVrYqjuPzIjNuJ8NZUrA/640?wx_fmt=jpeg) 

像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。**这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。**

常见的算法包括：主成份分析（Principle Component Analysis， PCA），偏最小二乘回归（Partial Least Square Regression，PLS）， Sammon 映射，多维尺度（Multi-Dimensional Scaling, MDS）,  投影追踪（Projection Pursuit）等。

### **17. 集成算法：**

![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qIbOcBUA0wqFdIDVfCFFovOL3mhRpn46hclfIsXWwrdRyibULic8PlianQ/640?wx_fmt=jpeg)

集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。

这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，堆叠泛化（Stacked Generalization， Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。

**常见机器学习算法优缺点：**

**朴素贝叶斯：**

1. 如果给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。

2. 计算公式如下：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qB4xCrGUgZhDHibD4aP4OGQs9ytWy0DJXjcE7HNNoWOTTgbqs3S2j2FQ/640?wx_fmt=png) 

其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qibv9ibweb3fQemic0UK0OIAI2iatkPB96ticBkCaGDicNTfTSicGKrEy9nKWw/640?wx_fmt=png)的计算方法，而由朴素贝叶斯的前提假设可知，![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q11MexVkIGnj5bXktGkRLCxVjDQEAicDAoamicoRoaRz6fePCYMib1jzbw/640?wx_fmt=png) =![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qcvE7Yem31uby5MibqeyGRAREd4W7zvhGODjhmdIPcrrRXIdOjjFRKEA/640?wx_fmt=png) ，因此一般有两种，一种是在类别为 ci 的那些样本集中，找到 wj 出现次数的总和，然后除以该样本的总和；第二种方法是类别为 ci 的那些样本集中，找到 wj 出现次数的总和，然后除以该样本中所有特征出现次数的总和。

3. 如果 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qibv9ibweb3fQemic0UK0OIAI2iatkPB96ticBkCaGDicNTfTSicGKrEy9nKWw/640?wx_fmt=png)中的某一项为 0，则其联合概率的乘积也可能为 0，即 2 中公式的分子为 0，为了避免这种现象出现，一般情况下会将这一项初始化为 1，当然为了保证概率相等，分母应对应初始化为 2（这里因为是 2 类，所以加 2，如果是 k 类就需要加 k，术语上叫做 laplace 光滑, 分母加 k 的原因是使之满足全概率公式）。

**朴素贝叶斯的优点：**对小规模的数据表现很好，适合多分类任务，适合增量式训练。

**缺点：**对输入数据的表达形式很敏感。

**决策树：**决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。

信息熵的计算公式如下:

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qmkSrNxFPSK3dQGibnhEur7JVe1xs7Rm6Ncf9XLuQtyT5EeMkiblrEwUw/640?wx_fmt=png) 

其中的 n 代表有 n 个分类类别（比如假设是 2 类问题，那么 n=2）。分别计算这 2 类样本在总样本中出现的概率 p1 和 p2，这样就可以计算出未选中属性分枝前的信息熵。

现在选中一个属性 xi 用来进行分枝，此时分枝规则是：如果 xi=vx 的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括 2 个类别，分别计算这 2 个分支的熵 H1 和 H2, 计算出分枝后的总信息熵 H’=p1*H1+p2*H2.，则此时的信息增益ΔH=H-H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性。

决策树的优点：计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；

缺点：**容易过拟合（后续出现了随机森林，减小了过拟合现象）。**

**Logistic 回归：**Logistic 是用来分类的，是一种线性分类器，需要注意的地方有：

**1. logistic 函数表达式为：**

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qOBnzKnedXLiaWNsbibSXkq9QM34suqcF9L3l9Z1TypFD7YYbKajDrIjg/640?wx_fmt=png) 

其导数形式为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q40SPesMEf0SrR84CY4gbWWGmOO8I5YLvvW7xqUWxuibUAQq74d9xHNQ/640?wx_fmt=png) 

2. logsitc 回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qwoz8Fg3t5lmLxicfeXc2DDR67mP99jj8Kru7uLOZsJLDcIhPhJpn0SA/640?wx_fmt=png) 

到整个样本的后验概率：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qvSsXbSot5QIibt6sfgw4un0JRTqV8YIeNWKUWqgcYQEWRGxHvNzR4Yg/640?wx_fmt=png) 

其中：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q2xjejoY6EEhbWf4CqL4H46dOd5cWPUXq6N1icueohfKrRCrxQHR7micA/640?wx_fmt=png) 

通过对数进一步化简为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q8NNz58LUN720zTWBpYBwvSGhnO72coShneOCJIDtYI26xttIQZPeWQ/640?wx_fmt=png) 

3. 其实它的 loss function 为 - l(θ)，因此我们需使 loss function 最小，可采用梯度下降法得到。梯度下降法公式为:

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qKMO4q9nhBK9ZTyQa7nWZa1dHxWPZbRvib0yt3S0gyJJZN4YEibgwQTww/640?wx_fmt=png) 

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qyesIC2sKYFW3icrlicGTuMrJibib9eViac1VmrWKFpjFU09QtxLy1XTcWHg/640?wx_fmt=png) 

Logistic 回归优点：

1. 实现简单

2. 分类时计算量非常小，速度很快，存储资源低；

缺点：

1. 容易欠拟合，一般准确度不太高

2. 只能处理两分类问题（在此基础上衍生出来的 softmax 可以用于多分类），且必须线性可分；

**线性回归：**  

线性回归才是真正用于回归的，而不像 logistic 回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用 normal equation 直接求得参数的解，结果为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qIS1xC4wzibxJ2ib4s87ibKJqqNeNS3KFVjy65GJsWXXlhwI0eaicT5HJHA/640?wx_fmt=png) 

而在 LWLR（局部加权线性回归）中，参数的计算表达式为:

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qxIyuNtqvPQ3R3Q4g8lQDqP1cMslIXnTcllSGrkzgus2bfRFPwE8uqQ/640?wx_fmt=png) 

因为此时优化的是：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qFBzx66EwNj6xDqyXw3eQmOQOpOYPLSG8xbk0f0xQZUMT3N33pZ4tng/640?wx_fmt=png) 

由此可见 LWLR 与 LR 不同，LWLR 是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。

线性回归优点：实现简单，计算简单；

缺点：不能拟合非线性数据；

 **KNN 算法：**KNN 即最近邻算法，其主要过程为：

1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；  

2. 对上面所有的距离值进行排序；

3. 选前 k 个最小距离的样本；

4. 根据这 k 个样本的标签进行投票，得到最后的分类类别；

如何选择一个最佳的 K 值，这取决于数据。一般情况下，在分类时较大的 K 值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的 K 值可通过各种启发式技术来获取，比如，交叉验证。**另外噪声和非相关性特征向量的存在会使 K 近邻算法的准确性减小。**

近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的 K 值，K 近邻保证错误率不会超过贝叶斯理论误差率。

注：马氏距离一定要先给出样本集的统计性质，比如均值向量，协方差矩阵等。关于马氏距离的介绍如下：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qqh28r8PmU5VPM8Q2szTyxJxrm5FcWkthK4UqmBicMwctrV80iblWicZgA/640?wx_fmt=png) 

**KNN 算法的优点：**

1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；

2. 可用于非线性分类；

3. 训练时间复杂度为 O(n)；

4. 准确度高，对数据没有假设，对 outlier 不敏感；

缺点：

1. 计算量大；

2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；

3. 需要大量的内存；

**SVM：**

要学会如何使用 libsvm 以及一些参数的调节经验，另外需要理清楚 svm 算法的一些思路：

1. svm 中的最优分类面是对所有样本的几何裕量最大（为什么要选择最大间隔分类器，请从数学角度上说明？网易深度学习岗位面试过程中有被问到。答案就是几何间隔与样本的误分次数间存在关系：![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qOz8sgawOJowjcZ2sE2VLjOwfNDiaDRbauLovvibibkoA9ynkCFibSHpdgw/640?wx_fmt=png) ，其中的分母就是样本到分类间隔距离，分子中的 R 是所有样本中的最长向量值），即：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qb0jibYQyCibMOjPqv7z8cGcGkDSJMThIEeurJkQyyE9BPjbPYe2sDiaPw/640?wx_fmt=png) 

经过一系列推导可得为优化下面原始目标：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qw7oHL1la222uicInib7Zn5gJu4hECppTtkYT0KExiaRAGSbluFqa2Tllg/640?wx_fmt=png) 

2. 下面来看看拉格朗日理论：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q69pw82FHbkhFeL3NicaHYAMfXZf9unBwtJkqgp95pURHT7ICzBI3PeA/640?wx_fmt=png) 

可以将 1 中的优化目标转换为拉格朗日的形式（通过各种对偶优化，KKD 条件），最后目标函数为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qhEjYoQicqTXlV40u7CQ5oMjLV6lHXQgDDrmsHfgHbGKMKX2rVyFibIGw/640?wx_fmt=png) 

我们只需要最小化上述目标函数，其中的α为原始优化问题中的不等式约束拉格朗日系数。

3. 对 2 中最后的式子分别 w 和 b 求导可得：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2quNUKBdaUL6VeRWaeEoHdoMdWe3xptEhy8TcvMsELb4m9WCr1j7mU4g/640?wx_fmt=png) 

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qLyBKFmYJMicaFCWOHIDxnSSiaBMjLAkibyDiaibvttwudKt8j7dkhBbb1EA/640?wx_fmt=png) 

由上面第 1 式子可以知道，如果我们优化出了α，则直接可以求出 w 了，即模型的参数搞定。而上面第 2 个式子可以作为后续优化的一个约束条件。

4. 对 2 中最后一个目标函数用对偶优化理论可以转换为优化下面的目标函数：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qcASVibJMjGKc6UQoMmNL3ldyI8pNRtUgz5aXoJlxUWpKITgUDia9ribAw/640?wx_fmt=png) 

而这个函数可以用常用的优化方法求得α，进而求得 w 和 b。

5. 按照道理，svm 简单理论应该到此结束。不过还是要补充一点，即在预测时有：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q4vgpOjeWAfuguQadicIh7OeuPV9gtgopA0sYB1NOIuKSvl0VulxiaDsA/640?wx_fmt=png) 

那个尖括号我们可以用核函数代替，这也是 svm 经常和核函数扯在一起的原因。

6. 最后是关于松弛变量的引入，因此原始的目标优化公式为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qHCatjCA3T5KyCsj14HfruRNwe8zJAeMrEFaLLicd0W1ljQvaGFRLZXQ/640?wx_fmt=png) 

此时对应的对偶优化公式为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q6us1tUWvWJoRKACDice7GvDJjibVpuhuCgfkdK5KuL1KQfbD5DrlSpgQ/640?wx_fmt=png) 

与前面的相比只是α多了个上界。

**SVM 算法优点：**

1. 可用于线性 / 非线性分类，也可以用于回归；

2. 低泛化误差；

3. 容易解释；

4. 计算复杂度较低；

缺点：

1. 对参数和核函数的选择比较敏感；

2. 原始的 SVM 只比较擅长处理二分类问题；

Boosting：  

主要以 Adaboost 为例，首先来看看 Adaboost 的流程图，如下：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qvxrIOaVF66nrWT1KItC3Ong8Eag07sgApc7k2zrWVbiabUDNjcibHM1A/640?wx_fmt=png) 

从图中可以看到，在训练过程中我们需要训练出多个弱分类器（图中为 3 个），每个弱分类器是由不同权重的样本（图中为 5 个训练样本）训练得到（其中第一个弱分类器对应输入样本的权值是一样的），而每个弱分类器对最终分类结果的作用也不同，是通过加权平均输出的，权值见上图中三角形里面的数值。那么这些弱分类器和其对应的权值是怎样训练出来的呢？

下面通过一个例子来简单说明，假设的是 5 个训练样本，每个训练样本的维度为 2，在训练第一个分类器时 5 个样本的权重各为 0.2. 注意这里样本的权值和最终训练的弱分类器组对应的权值α是不同的，样本的权重只在训练过程中用到，而α在训练过程和测试过程都有用到。

现在假设弱分类器是带一个节点的简单决策树，该决策树会选择 2 个属性（假设只有 2 个属性）的一个，然后计算出这个属性中的最佳值用来分类。

**Adaboost 的简单版本训练过程如下：**

1. 训练第一个分类器，样本的权值 D 为相同的均值。通过一个弱分类器，得到这 5 个样本（请对应书中的例子来看，依旧是 machine learning in action）的分类预测标签。与给出的样本真实标签对比，就可能出现误差 (即错误)。如果某个样本预测错误，则它对应的错误值为该样本的权重，如果分类正确，则错误值为 0. 最后累加 5 个样本的错误率之和，记为ε。

2. 通过ε来计算该弱分类器的权重α，公式如下：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qFg8C6BPiaJbdiaWyMLM2TV5vIIPYc5GH4OOfFkj3XdeVqO9BkFM6YCnw/640?wx_fmt=png) 

3. 通过α来计算训练下一个弱分类器样本的权重 D，如果对应样本分类正确，则减小该样本的权重，公式为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qFZoQJKspMsjKh6e9O4ic2xxWw37e1JKm8iacexRriat9Szw8rtq6IDjgw/640?wx_fmt=png) 

如果样本分类错误，则增加该样本的权重，公式为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qaruSLadPydFjzJHjwb7HNiaGx55xyLkOUfWtbZJQN4IjxoGlyupQucw/640?wx_fmt=png) 

4. 循环步骤 1,2,3 来继续训练多个分类器，只是其 D 值不同而已。

测试过程如下：

输入一个样本到训练好的每个弱分类中，则每个弱分类都对应一个输出标签，然后该标签乘以对应的α，最后求和得到值的符号即为预测标签值。

**Boosting 算法的优点：**

1. 低泛化误差；

2. 容易实现，分类准确率较高，没有太多参数可以调；

3. 缺点：

4. 对 outlier 比较敏感；

聚类：  

根据聚类思想划分：

1. 基于划分的聚类:

K-means, k-medoids(每一个类别中找一个样本点来代表),CLARANS.

k-means 是使下面的表达式值最小：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q2B1ZZ5iagjbReBWjgW2BABAlJ4JbGZje3kicb6HI04rcFTiabuIfO0GfQ/640?wx_fmt=png) 

k-means 算法的优点：

（1）k-means 算法是解决聚类问题的一种经典算法，算法简单、快速。

（2）对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是 O(nkt)，其中 n 是所有对象的数目，k 是簇的数目, t 是迭代的次数。通常 k<<n。这个算法通常局部收敛。

（3）算法尝试找出使平方误差函数值最小的 k 个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。

缺点：

（1）k - 平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合。

（2）要求用户必须事先给出要生成的簇的数目 k。

（3）对初值敏感，对于不同的初始值，可能会导致不同的聚类结果。

（4）不适合于发现非凸面形状的簇，或者大小差别很大的簇。

（5）对于 "噪声" 和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。

2. 基于层次的聚类：

自底向上的凝聚方法，比如 AGNES。

自上向下的分裂方法，比如 DIANA。

3. 基于密度的聚类：DBSACN,OPTICS,BIRCH(CF-Tree),CURE.

4. 基于网格的方法：STING, WaveCluster.

5. 基于模型的聚类：EM,SOM,COBWEB.

推荐系统：推荐系统的实现主要分为两个方面：基于内容的实现和协同滤波的实现。

**基于内容的实现：**不同人对不同电影的评分这个例子，可以看做是一个普通的回归问题，因此每部电影都需要提前提取出一个特征向量 (即 x 值)，然后针对每个用户建模，即每个用户打的分值作为 y 值，利用这些已有的分值 y 和电影特征值 x 就可以训练回归模型了 (最常见的就是线性回归)。

这样就可以预测那些用户没有评分的电影的分数。（值得注意的是需对每个用户都建立他自己的回归模型）

从另一个角度来看，也可以是先给定每个用户对某种电影的喜好程度（即权值），然后学出每部电影的特征，最后采用回归来预测那些没有被评分的电影。

当然还可以是同时优化得到每个用户对不同类型电影的热爱程度以及每部电影的特征。

基于协同滤波的实现：协同滤波（CF）可以看做是一个分类问题，也可以看做是矩阵分解问题。协同滤波主要是基于每个人自己的喜好都类似这一特征，它不依赖于个人的基本信息。

比如刚刚那个电影评分的例子中，预测那些没有被评分的电影的分数只依赖于已经打分的那些分数，并不需要去学习那些电影的特征。

**SVD 将矩阵分解为三个矩阵的乘积，**公式如下所示：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qxoqImhrqcqRnjgF6qHibWnTlJibyFiaUYnT7XiaBcaku4SNmUnN9qB1sGw/640?wx_fmt=png) 

中间的矩阵 sigma 为对角矩阵，对角元素的值为 Data 矩阵的奇异值 (注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。如下图所示：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qpZTia9Nu80TIE4kYibtKicvCGiayXyBHAMZ4G3icrzjFYmeXbqVcc6fsQyA/640?wx_fmt=png) 

其中更深的颜色代表去掉小特征值重构时的三个矩阵。

果 m 代表商品的个数，n 代表用户的个数，则 U 矩阵的每一行代表商品的属性，现在通过降维 U 矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为 k 维）。这样当新来一个用户的商品推荐向量 X，则可以根据公式 X'*U1*inv(S1) 得到一个 k 维的向量，然后在 V’中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。

 pLSA：**由 LSA 发展过来，而早期 LSA 的实现主要是通过 SVD 分解。**pLSA 的模型图如下

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qichmc9crI0buZ8JYBmNrhlKngkNInw5TXKIGfTpUtztIEzu4oPiboDRg/640?wx_fmt=png) 

公式中的意义如下：

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qT21et5qFuib5Hu5xrCicMDZ9Dd1zxX6qibFMu6piaCVM5shuwZW2UvdVLQ/640?wx_fmt=jpeg) 

LDA 主题模型，概率图如下：

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qfntmeA7puuranIASudZEyyfONMaN3TPzpmqfN5FjRSKTlvLuRujI7A/640?wx_fmt=jpeg) 

和 pLSA 不同的是 LDA 中假设了很多先验分布，且一般参数的先验分布都假设为 Dirichlet 分布，其原因是共轭分布时先验概率和后验概率的形式相同。

GDBT：GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，好像在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），它是一种迭代的决策树算法，**该算法由多棵决策树组成，所有树的输出结果累加起来就是最终答案。**

它在被提出之初就和 SVM 一起被认为是泛化能力（generalization) 较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。

GBDT 是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。为了防止过拟合，和 Adaboosting 一样，也加入了 boosting 这一项。

Regularization 作用是

1. 数值上更容易求解；

2. 特征数目太大时更稳定；

3. 控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。

4. 减小参数空间；参数空间越小，复杂度越低。

5. 系数越小，模型越简单，而模型越简单则泛化能力越强（Ng 宏观上给出的解释）。

6. 可以看成是权值的高斯先验。

**异常检测：**可以估计样本的密度函数，对于新样本直接计算其密度，如果密度值小于某一阈值，则表示该样本异常。而密度函数一般采用多维的高斯分布。

如果样本有 n 维，则每一维的特征都可以看作是符合高斯分布的，即使这些特征可视化出来不太符合高斯分布，也可以对该特征进行数学转换让其看起来像高斯分布，比如说 x=log(x+c), x=x^(1/c) 等。异常检测的算法流程如下：

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qmrxfwqHXYdKKKSWTUXGqRqf4mH2K0jJSPIjpANroaW7IN04jyIQ61w/640?wx_fmt=jpeg) 

其中的ε也是通过交叉验证得到的，也就是说在进行异常检测时，前面的 p(x) 的学习是用的无监督，后面的参数ε学习是用的有监督。那么为什么不全部使用普通有监督的方法来学习呢（即把它看做是一个普通的二分类问题）？

主要是因为在异常检测中，异常的样本数量非常少而正常样本数量非常多，因此不足以学习到好的异常行为模型的参数，因为后面新来的异常样本可能完全是与训练样本中的模式不同。

EM 算法：**有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计**，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用 EM 算法来求模型的参数的（对应模型参数个数可能有多个），

**EM 算法一般分为 2 步：**

**E 步：**选取一组参数，求出在该参数下隐含变量的条件概率值；

**M 步：**结合 E 步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。

重复上面 2 步直至收敛，公式如下所示：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qJryhbRHsjEib87mPEH5MNkAOicpcJqjc0veBd0iaCC5xsW0JLEku82JWg/640?wx_fmt=png) 

M 步公式中下界函数的推导过程：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q1sv0ZjDXbCDCibTRQFSB0p6vugib8sEdMRgwpFibHb8IWb2QTrr9ETjeQ/640?wx_fmt=png) 

EM 算法一个常见的例子就是 GMM 模型，每个样本都有可能由 k 个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k 个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。

GMM 的 E 步公式如下（计算每个样本对应每个高斯的概率）：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qnhhI1YfDUgySEbQZRkgt6ic7LLib2VDuBn3OxB838ze89GJ2VEic7yWeA/640?wx_fmt=png) 

更具体的计算公式为：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qV82zyR43NyGL8iaQic0ygt5UvOorrybibOdjn7Eb7OaicyQPr4DMDzVbSQ/640?wx_fmt=png) 

M 步公式如下（计算每个高斯的比重，均值，方差这 3 个参数）：

 ![](https://mmbiz.qpic.cn/mmbiz_png/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qBYuX8Rfiboz5TkuWY6bF0hqZo56Xr7G2DKKhiclzlUE3Q9JPLYib1hfOQ/640?wx_fmt=png) 

Apriori 是关联分析中比较早的一种方法，主要用来挖掘那些频繁项集合。其思想是：

1. 如果一个项目集合不是频繁集合，那么任何包含它的项目集合也一定不是频繁集合；

2. 如果一个项目集合是频繁集合，那么它的任何非空子集也是频繁集合；

Aprioir 需要扫描项目表多遍，从一个项目开始扫描，舍去掉那些不是频繁的项目，得到的集合称为 L，然后对 L 中的每个元素进行自组合，生成比上次扫描多一个项目的集合，该集合称为 C，接着又扫描去掉那些非频繁的项目，重复…

看下面这个例子，元素项目表格：

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qFSiarUvJk1prbKG8a9icKH7NGqko1FD9gntBVSAQzUjKibCfF4mdtC5CA/640?wx_fmt=jpeg) 

如果每个步骤不去掉非频繁项目集，则其扫描过程的树形结构如下：

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qRicVpsyEibpd4Ka6TpgQYL0DycicZbL3wxpEuq5wTVoHD4A3dcotibyxkw/640?wx_fmt=jpeg) 

在其中某个过程中，可能出现非频繁的项目集，将其去掉（用阴影表示）为：

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2quribfdQBPK4DbmbDLhtfDhkdUibh8gvhXs16rh7rnoCIDhzp0kiaZKibAQ/640?wx_fmt=jpeg) 

FP Growth 是一种比 Apriori 更高效的频繁项挖掘方法，它只需要扫描项目表 2 次。其中第 1 次扫描获得当个项目的频率，去掉不符合支持度要求的项，并对剩下的项排序。第 2 遍扫描是建立一颗 FP-Tree(frequent-patten tree)。

接下来的工作就是在 FP-Tree 上进行挖掘，比如说有下表：

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2qYK5v1TMq1ezticpxcbLc9QADZElWic9zardzr1jUqG9lqyjGmic6gx2Gw/640?wx_fmt=jpeg) 

它所对应的 FP_Tree 如下：

 ![](https://mmbiz.qpic.cn/mmbiz_jpg/hN1l83J6Ph8LGibicFIKBZPjYB4micHcic2q4lKicOjya5c3IyFIndUJGGAVtwZtMPkOcwSCrozRCdGIhef2vpmOarQ/640?wx_fmt=jpeg) 

然后从频率最小的单项 P 开始，找出 P 的条件模式基，用构造 FP_Tree 同样的方法来构造 P 的条件模式基的 FP_Tree，在这棵树上找出包含 P 的频繁项集。

依次从 m,b,a,c,f 的条件模式基上挖掘频繁项集，有些项需要递归的去挖掘，比较麻烦，比如 m 节点。