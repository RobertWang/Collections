> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [mp.weixin.qq.com](https://mp.weixin.qq.com/s/1K-waX2TMsV4hk4QYh2YVA)

最近大模型发布的很频繁，各种排行榜几乎天天在变化，你方唱罢我登场，好不热闹。

我现在有一张老破小的显卡，看到新发布了个模型，如何根据参数量粗略判断能不能运行起来？

只进行推理
=====

如果只是进行推理的话，还是比较容易计算的。

目前模型的参数绝大多数都是`float32`类型, 占用 4 个字节。所以一个粗略的计算方法就是，**每 10 亿个参数，占用 4G 显存** (实际应该是`10^9*4/1024/1024/1024=3.725G`，为了方便可以记为 4G)。

比如 LLaMA 的参数量为 7000559616，那么全精度加载这个模型参数需要的显存为：

```
7000559616 * 4 /1024/1024/1024 = 26.08G


```

这个数字就有点尴尬，专注游戏的 Geforce 系列最高只有 24G，运行不起来，至少得上 Tesla 系列了。

![](https://mmbiz.qpic.cn/mmbiz_png/GCNbdU0ticibrngUbcakTvwMIJjQwTDAIWG9Speb4X3oLTLNnDiaNE84qHFoE6iczhD0r24qib6Xo05DM2A0Sn4Fudw/640?wx_fmt=png)

好在我们可以才用半精度的`FP16/BF16`来加载，这样每个参数只占 2 个字节，所需显存就降为一半，只需要 13.04G。游戏学习两不误。

半精度是个不错的选择，显存少了一半，模型效果因为精度的原因会略微降低，但一般在可接受的范围之内。

如果有个 3070 显卡，8G 显存，还有的玩么？可以玩，采用 int8 的精度，显存再降一半，仅需 6.5G，但是模型效果会更差一些。

我的 PC 机大概 10 年前配的，GTX 960 4G，当时最顶配！然而现在只能玩玩 int4 精度，显存再降一半，仅需 3.26G。当年我可是花了千把块买的，虽说只能勉强推理 7B 的大模型，但是也算是上车了。

目前 int4 就是最低精度了，再往下效果就很难保证了。比如百川给的量化结果对比如下：

![](https://mmbiz.qpic.cn/mmbiz_png/GCNbdU0ticibrngUbcakTvwMIJjQwTDAIWXpiaOdWoNs36EUIaXY1eEqP2BYsNrjJM4XByhAk8lnQjmOeWGQjxeuA/640?wx_fmt=png)

注意上面只是加载模型到显存，模型运算时的一些临时变量也需要申请空间，比如你 beam search 的时候。所以真正做推理的时候记得留一些 Buffer，不然就容易 OOM。

如果显存还不够，就只能采用 Memery Offload 的技术，把部分显存的内容给挪到内存，但是这样会显著降低推理速度。

懒人记法表 (粗估，有误差)

<table><thead><tr data-style="border-width: 1px 0px 0px; border-right-style: initial; border-bottom-style: initial; border-left-style: initial; border-right-color: initial; border-bottom-color: initial; border-left-color: initial; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: white;"><th data-style="border-top-width: 1px; border-color: rgb(204, 204, 204); text-align: left; background-color: rgb(240, 240, 240); min-width: 85px;">dtype</th><th data-style="border-top-width: 1px; border-color: rgb(204, 204, 204); text-align: left; background-color: rgb(240, 240, 240); min-width: 85px;" class="">每 10 亿参数需要占用内存</th></tr></thead><tbody><tr data-style="border-width: 1px 0px 0px; border-right-style: initial; border-bottom-style: initial; border-left-style: initial; border-right-color: initial; border-bottom-color: initial; border-left-color: initial; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: white;"><td data-style="border-color: rgb(204, 204, 204); min-width: 85px;">float32</td><td data-style="border-color: rgb(204, 204, 204); min-width: 85px;" class="">4G</td></tr><tr data-style="border-width: 1px 0px 0px; border-right-style: initial; border-bottom-style: initial; border-left-style: initial; border-right-color: initial; border-bottom-color: initial; border-left-color: initial; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: rgb(248, 248, 248);"><td data-style="border-color: rgb(204, 204, 204); min-width: 85px;">fp16/bf16</td><td data-style="border-color: rgb(204, 204, 204); min-width: 85px;">2G</td></tr><tr data-style="border-width: 1px 0px 0px; border-right-style: initial; border-bottom-style: initial; border-left-style: initial; border-right-color: initial; border-bottom-color: initial; border-left-color: initial; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: white;"><td data-style="border-color: rgb(204, 204, 204); min-width: 85px;">int8</td><td data-style="border-color: rgb(204, 204, 204); min-width: 85px;" class="">1G</td></tr><tr data-style="border-width: 1px 0px 0px; border-right-style: initial; border-bottom-style: initial; border-left-style: initial; border-right-color: initial; border-bottom-color: initial; border-left-color: initial; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: rgb(248, 248, 248);"><td data-style="border-color: rgb(204, 204, 204); min-width: 85px;">int4</td><td data-style="border-color: rgb(204, 204, 204); min-width: 85px;">0.5G</td></tr></tbody></table>

我还想训练
=====

训练的话就有点复杂了，因为模型训练的时候显存使用包括如下几部分：

1.  模型权重，计算方法和上面一样。
    
2.  优化器。
    

*   如果你采用 AdamW，每个参数需要占用 8 个字节，因为需要维护两个状态。也就说优化器使用显存是全精度 (float32) 模型权重的 2 倍。
    
*   如果采用 bitsandbytes 优化的 AdamW，每个参数需要占用 2 个字节，也就是全精度 (float32) 模型权重的一半。
    
*   如果采用 SGD，则优化器占用显存和全精度模型权重一样。
    

4.  梯度
    

*   梯度占用显存和全精度 (float32) 模型权重一样。
    

6.  计算图内部变量 (有时候也叫 Forward Activations)
    

*   pytorch/tensorflow 等框架采用图结构来计算，图节点在 forward 和 backward 的时候需要存储，所以也需要占用显存。
    

![](https://mmbiz.qpic.cn/mmbiz_gif/GCNbdU0ticibrngUbcakTvwMIJjQwTDAIWeDL5kCzVMeaiaRUGjhV2MShqpNxREVX0lq1nXiahcibNLJmQrRpjyTLYQ/640?wx_fmt=gif)

*   比如下面代码
    
    ```
    y = self.net_a(x)
    z = self.net_b(y)
    
    
    ```
    
*   这里面中间的 x, y, z 都需要存储，但是如果写成下面这样，y 就不用存储了。
    
    ```
    z = self.net_b(selt.net_a(x))
    
    
    ```
    
*   理论上一个 net block 可以完全用函数给包裹起来，不使用中间变量。下一代计算框架是函数式编程语言？
    
*   所以这一部分跟模型具体的实现有关系，而且 **正比于 batch_size** 。batch_size 越大，这一部分占用的越多。同样的结论也适用于 sequence length。
    

10.  一些临时显存占用，先不计算。
    

所以说，**如果模型想要训练，只看前 3 部分，需要的显存是至少推理的 3-4 倍。**7B 的全精度模型加载需要 78G ~ 104G。

然后**计算图内部变量这一部分只能在运行时候观测**了，可以两个不同的 batch 的占用显存的差值大概估算出来。

以上就是根据模型参数估计显存的大概方法，实际影响显存占用的因素还有很多，所以只能粗略估计个数量级。

剖析完训练时显存占用情况后，优化的思路也就有了，目前市面上主流的一些计算加速的框架如 DeepSpeed, Megatron 等都在降低显存方面做了很多优化工作，比如量化，模型切分，混合精度计算，Memory Offload 等等，大家感兴趣后续可以再给大家分享。