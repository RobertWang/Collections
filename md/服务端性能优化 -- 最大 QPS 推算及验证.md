> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.cnblogs.com](https://www.cnblogs.com/huangyingsheng/p/13744422.html)

服务端性能优化 -- 最大 QPS 推算及验证
=======================

影响 QPS（即吞吐量）的因素有哪些？每个开发都有自己看法，一直以为众说纷纭，例如：

*   QPS 受编程语言的影响。（PHP 是最好的语言？）
*   QPS 主要受编程模型的影响，比如不是 coroutine、是不是 NIO、有没有阻塞？
*   QPS 主要由业务逻辑决定，业务逻辑越复杂，QPS 越低。
*   QPS 受数据结构和算法的影响。
*   QPS 受线程数的影响。
*   QPS 受系统瓶颈的影响。
*   QPS 和 RT 关系非常紧密。
*   more...

嗯，这些说法好像都对，但是好像又有点不对，好像总是不太完整，有没有一个系统点的说法能让人感觉一听就豁然开朗？  
今天我们就这个话题来阐述一下，将一些现有的理论作为依据，把上方这些看起来比较零碎的看法总结归纳起来，希望能为服务端的性能提升进行一点优化，这也是一个优化的起点，未来才有可能做更多的优化，例如 TCP、DNS、CDN、系统监控、多级缓存、多机房部署等等优化的手段。

**好了，废话不多说，直接开聊。**

我们经常再做优化的时候，例如电商的促销秒杀等活动页，一开始可能会认为说 Gzip 并不是影响 CPU 的最大因子，直到拿出一次又一次的实验数据，研发们才开始慢慢接受（尬不尬），这是为什么？难道说 Gzip 真的是影响 CPU 的最大因子吗？那我们就拿出一点数据来验证一下对吧，接下来我们从 RT 着手开始慢慢了解，看到文章结尾就知道为什么 Gzip 和 CPU 的关系，同事也会发现，性能优化的相关知识其实也是体系化的，并不是分散零碎的。

### RT

什么是 RT ？是概念还是名词还是理论？  
RT 其实也没那么玄乎，就是 Response Time （就是响应时间嘛，哈哈哈），只不过看你目前在什么场景下，也许你是 c 端（app、pc 等）的用户，响应时间是你请求服务器到服务器响应你的时间间隔，对于我们后端优化来说，就是接受到请求到响应用户的时间间隔。这听起来怎么感觉这不是在说废话吗？这说的不都是服务端的处理时间吗？不同在哪里？其实这里有个容易被忽略的因素，叫做网络开销。  
所以服务端 RT ≈ 网络开销 + 客户端 RT。也就是说，一个差的网络环境会导致两个 RT 差距的悬殊（比如，从深圳访问上海的请求 RT，远大于上海本地内的请求 RT）

客户端的 RT 则会直接影响客户体验，要降低客户端 RT，提升用户的体验，必须考虑两点，第一点是服务端的 RT，第二点是网络。对于网络来说常见的有 CDN、AND、专线等等，分别适用于不同的场景，有机会写个 blog 聊一下这个话题。

对于服务端 RT 来说，主要看服务端的做法。  
有个公式：RT = Thread CPU Time + Thread Wait Time  
从公式中可以看出，要想降低 RT，就要降低 Thread CPU Time 或者 Thread Wait Time。这也是马上要重点深挖的一个知识点。

**Thread CPU Time（简称 CPU Time）  
Thread Wait Time（简称 Wait Time）**

### 单线程 QPS

我们都知道 RT 是由两部分组成 CPU Time + Wait Time 。那如果系统里只有一个线程或者一个进程并且进程中只有一个线程的时候，那么最大的 QPS 是多少呢？  
假设 RT 是 199ms （CPU Time 为 19ms ，Wait Time 是 180ms ），那么 1000s 以内系统可以接收的最大请求就是  
1000ms/(19ms+180ms)≈5.025。

所以得出单线程的 QPS 公式：

$$单线程QPS = 1000ms/RT$$

### 最佳线程数

还是上面的那个话题 （CPU Time 为 19ms ，Wait Time 是 180ms ），假设 CPU 的核数 1。假设只有一个线程，这个线程在执行某个请求的时候，CPU 真正花在该线程上的时间就是 CPU Time，可以看做 19ms，那么在整个 RT 的生命周期中，还有 180ms 的 Wait Time，CPU 在做什么呢？抛开系统层面的问题（这里不考虑什么时间片轮循、上下文切换等等），可以认为 CPU 在这 180ms 里没做什么，至少对于当前的业务来说，确实没做什么。

*   一核的情况  
    由于每个请求的接收，CPU 只需要工作 19ms，所以在 180ms 的时间内，可以认为系统还可以额外接收 180ms/19ms≈9 个的请求。由于在同步模型中，一个请求需要一个线程来处理，因此，我们需要额外的 9 个线程来处理这些请求。这样，总的线程数就是：

$$（180ms + 19ms）/19ms ≈ 10个$$

        多线程之后，CPU Time 从 19ms 变成了 20ms，这 1ms 的差值代表多线程之后上下文切换、GC 带来的额外开销（对于我们 java 来说是 jvm，其他语言另外计算），这里的 1ms 只是代表一个概述，你也可以把它看做 n。

*   两核的情况  
    一核的情况下可以有 10 个线程，那么两核呢？在理想的情况下，可以认为最佳线程数为：2 x (180ms + 20ms)/20ms = 20 个
    
*   CPU 利用率  
    我们之前说的都是 CPU 满载下的情况，有时候由于某个瓶颈，导致 CPU 不得不有效利用，比如两核的 CPU，因为某个资源，只能各自使用一半的能效，这样总的 CPU 利用率就变成了 50%，再这样的情况下，最佳线程数应该是：50% x 2 x(180ms + 20ms)/20ms = 10 个  
    这个等式转换成公式就是：最佳线程数 = (RT/CPU Time) x CPU 核数 x CPU 利用率  
    当然，这不是随便推测的，在收集到的很多的一些著作或者论坛的文档里都有这样的一些实验去论述这个公式或者这个说法是正确的。
    

### 最大 QPS

#### 1. 最大 QPS 公式推导

假设我们知道了最佳线程数，同时我们还知道每个线程的 QPS，那么线程数乘以每个线程的 QPS 既这台机器在最佳线程数下的 QPS。所以我们可以得到下图的推算。

![](https://summer-blog-images.oss-cn-shanghai.aliyuncs.com/qps_optimization/image_001.png)

我们可以把分子和分母去约数，如下图。

![](https://summer-blog-images.oss-cn-shanghai.aliyuncs.com/qps_optimization/image_002.png)

于是简化后的公式如下图.

![](https://summer-blog-images.oss-cn-shanghai.aliyuncs.com/qps_optimization/image_003.png)

从公式可以看出，决定 QPS 的时 CPU Time、CPU 核数和 CPU 利用率。CPU 核数是由硬件做决定的，很难操纵，但是 CPU Time 和 CPU 利用率与我们的代码息息相关。

虽然宏观上是正确的，但是推算的过程中还是有一点小小的不完美，因为多线程下的 CPU Time（比如高并发下的 GC 次数增加消耗更多的 CPU Time、线程上下文切换等等）和单线程的 CPU Time 是不一样的，所以会导致推算出来的结果有误差。

尤其是在同步模型下的相同业务逻辑中，单线程时的 CPU Time 肯定会比大量多线程的 CPU Time 小，但是对于异步模型来说，切换的开销会变得小很多，为什么？这里先卖个葫芦吧，看完本篇就知道了。

既然决定 QPS 的是 CPU Time 和 CPU 核数，那么这两个因子又是由谁来决定的呢？（越看越懵哈）

#### 2.CPU Time

终于讲到了 CPU Time，CPU Time 不只是业务逻辑所消耗的 CPU 时间，而是一次请求中所有环节上消耗的 CPU 时间之和。比如在 web 应用中，一个请求过来的 HTTP 的解析所消耗的 CPU 时间，是 CPU Time 的一部分。另外，这个请求中请求 RPC 的 encode 和 decode 所消耗的 CPU 时间也是 CPU Time 的一部分。

那么 CPU Time 是由哪些因素决定的呢？两个关键字：数据结构 + 算法。  
举一些例子吧

*   均摊问题
*   hash 问题
*   排序和查找问题
*   状态机问题
*   序列化问题

#### 3.CPU 利用率

CPU 利用率不高的情况时常发生，一下因素都会影响 CPU 的利用率，从而影响系统可以支持的最大 QPS。

##### 1) IO 能力

*   磁盘 IO
*   网络 IO  
    · 带宽，比如某大促压力测试时，由于某个应用放在 Tair 中的数据量大，导致 Tair 的机器网卡跑满。  
    · 网路链路，还是这次大促，借用了其他核心交换机下的机器，导致客户端 RT 明显增加。

##### 2) 数据库连接池（并发能力 = PoolWaitTime/RT(Client) x PoolSize）。

##### 3) 内存不足，GC 大量占用 CPU，导致给业务逻辑使用的 CPU 利用率下降，而且 GC 时还满足 Amdahl 定律锁定义的场景。

##### 4) 共享资源的竞争，比如各种锁策略 (读写锁、锁分离等)，各种阻塞队列，等等。

##### 5) 所依赖的其他后端服务 QPS 低造成的瓶颈。

##### 6) 线程数或者进程数，甚至编程模型 (同步模型，异步模型)。

在压力测试过程中，出现最多的就是网络 IO 层面的问题，GC 大量占用 CPU Time 之类的问题也经常出现。

#### 4.CPU 核数，Amdahl 定律，Gustafson 定律

##### 1)Amdahl 定律 (安达尔定律，不是达尔文定律！！！)

Amdahl 定律是用来描述可伸缩性的，什么是可伸缩性？说白了就是比如增加计算机资源，如 CPU、内存、宽带等，QPS 能够相应的进行改进。

既然 Amdahl 定律是描述可伸缩性的，那它是如何描述的呢？

Amdahl 在自己的论文中指出，可伸缩性是指在一个系统中，基于可并行化和串行化的组件各自所占的比例，当程序获得额外的计算资源 (如 CPU 或者内存等) 时，系统理论上能够获得的加速值(QPS 或者其他指标可以翻几倍)。用一个公式来表达，如果 F 表示必须串行化执行的比例，那么在一个 N 核处理器的机器中，加速：

$$Speedup \leq \frac{1}{F+\frac{1-F}{N}}$$

这个公式代表的意义是比较广泛的，在项目管理中也有一句类似的话：

一个女人生一个孩子要 9 个月，但是永远不可能让 9 个女人在一个月内就生一个孩子。  
我们根据这个例子套一个公式先，这里设 F=100%，9 个女人表示 N=9，于是就有 1/(100%+(1-100%)/9)=1，所以 9 个女人的加速比为 1，等于没有加速。

到这里，其实这个公式还只是描述了在增加资源的情况下系统的加速比，而不是在资源不变的情况下优化数据结构和算法之后带来的提升。优化数据结构和算法带来的提升要看前文中最大的 QPS 公式。不过这两个公式也不是完全没有联系的，在增加资源的情况下，它们的联系还是比较紧密的。

##### 2) Gustafson 定律（古斯塔夫森定律）

这个定律名字有点长，但这不是关键，关键的是，它是 Amdahl 定律的补充，公式为：

$$S(P) = P-α·(P-1)$$

P 是处理器的个数，α是串行时间占总执行时间的比例。  
生孩子的案例再次套上这个公式，P 为女人的个数，等于 9，串行比例是 100%。Speedup=9-100%x(9-1)=1，也就是 9 个女人是无法在一个月内把孩子生出来的……

之所以说是 Amdahl 定律的补充，是因为两个定律的关系是相辅相成的关系。前者从串行和并行执行时间的角度来推导，后者从串行和并行的计算量角度来推导，不管是哪个角度，最终的结果其实是一样的。

##### 3)CPU 核数和 Amdahl 定律的关系

通过最大 QPS 公式，我们发现，在 CPU Time 和 CPU 利用率不变的情况下，核数越多，QPS 就越大。比如核数从 1 到 4，在 CPU Time 和 CPU 利用率不变的情况下，加速比应该是 4，所以 QPS 应该也是增加 4 倍。

这是资源增加（CPU 核数增加）的情况下的加速比，也可以通过 Amdahl 定律来衡量，考虑串行和并行的比例在增加资源的情况下是否会改变。也就是要考虑在 N 增加的情况下，F 受哪些因素的影响：

$$Speedup \leq \frac{1}{F+\frac{1-F}{N}}$$

只要 F 大于 0，最大 QPS 就不会翻 4 倍。  
一个公式说要增加 4 被倍，一个定理说 没有 4 倍，互相矛盾？  
其实事情是这样的，通过最大 QPS 公式，我们可以发现，如果 CPU Time 和 CPU 利用率不变，核数从 1 增加到 4，QPS 会相应的增加 4 倍。但是在实际情况下，当核数增加时，CPU Time 和 CPU 利用率大部分时候是变化的，所以前面的假设不成立，即一般场景下 QPS 不能增加 4 倍。

而 Amdahl 定律中的 N 变化时，F 也可能会变化，即一般场景下，最大 QPS 并不能增加 4 被，所以这其实并不矛盾。相反它们是相辅相成的。这里一定要注意，这里说的是一般场景，如果你的场景完全没有串行（程序没有串行，系统没有串行，上下文切换没有串行，什么串行都没有），那么理论上是可以增加 4 倍的。

为什么增加计算机资源时，最大 QPS 公式中的 CPU Time 和 CPU 利用率会变化，F 也会变化呢？我们可以从宏观上分析一下，增加计算机资源时，达到满载:

*   QPS 会更高，单位时间内产生的对象会更多。在同等条件下，minor GC 被触发的次数增加，还有些场景发生过对象多到响应没返回它们就进了 “老年代”，从而 fullGC 被触发。宏观上，这是属于串行的部分，对于 Amdahl 公式来说 F 会受到影响，对于最大 QPS 公式来说，CPU Time 和 CPU 利用率也受到影响.
    
*   在同步模型下大量的线程在完成一次请求中，上下文被切换的次数大大增加。
    

*   尤其是在有 串行模块的时候，串行的执行和等待时间增加，F 会变化，某些场景下 CPU  
    利用率也达不到理想效果，这取决于你的代码。这也是要做锁分离、为什么要缩小同步  
    块的原因。当然还有锁自身的优化，比如偏向、自旋、读写分离等技术，都是为了不断  
    地减少 Amdahl 定律中的 F，也是为了减少 CPU Time (锁本身的优化)，提高 CPU 利  
    用率 (使用锁的方法的优化)。
    

。锁本身的优化最为津津乐道的是自旋、偏向、自适应，synchronized 分析，还有 reetrantLock 的代码及 AQS 等等。

。使用锁的优化方法最常见的是缩小锁区间、锁分离、无锁化、volatile。

所以在增加计算资源时，更高的并发产生，会引起最大 QPS 公式中两个参数的变化，也会  
引起 Amdahl 定律中 F 值的变化，同时公式和定律变化的趋势是相同的。Amdahl 定律是得到广  
泛认可的，也是得到数据验证的。最大 QPS 公式好像没有人验证过，这里引用一个比较有名的  
测试结果，如下图.

![](https://summer-blog-images.oss-cn-shanghai.aliyuncs.com/qps_optimization/image_005.png)

*   当计算资源 (处理器数量) 增加时，在串行部分比例不变的情况下，CPU 利用率下降。
*   当计算资源 (处理器数量) 增加时，串行占的比例越大，CPU 利用率下降得越多。

### 实验数据验证公式

所以其实到现在我们一直在说理论，带了一点点的公式，听起来好像是那么回事，但是公式到底怎么用？准不准确？可以精准测试还是概要测试即可？

我们接下来实验一下吧。

!!! 接下来会涉及到 CPU Time 包含了操作系统对 CPU 的消耗，比如进程，线程调度等。

#### 1. 数据准备

这里就用之前的一个电商活动页面的优化来说吧，在这个过程中，我们做了大量测试，由于测试中使用了 localhost 方式，所以 Java 进程在 IO 上的 Wait Time 是非常小的。接下来，由于最佳线程数接近 CPU 核数,  
所以在两核的机器上使用了 10 个 Java 进程，客户端发起了 10 个并发请求, 这是在最佳线程数下 (最佳线程数在一个区间里，在这个区间里 QPS 总体变化不大, 并且也用了 5、15 个并发测试效果，发现 QPS 值基本相同) 得出的大量结果，接下来就分析一下这些测试结果，见下表。

##### 1) 测试 QPS 结果

<table><thead><tr><th>原始页面大小</th><th>压缩后的大小</th><th>优化前 QPS</th><th>优化后 QPS</th><th>优化前 RT</th><th>优化后 RT</th></tr></thead><tbody><tr><td>92kb</td><td>17kb</td><td>164</td><td>2024</td><td>60.7ms</td><td>4.9ms</td></tr><tr><td>138kb</td><td>8.7kb</td><td>143</td><td>1859</td><td>69.8ms</td><td>3.3ms</td></tr><tr><td>182kb</td><td>11.4kb</td><td>121</td><td>2083</td><td>82.3ms</td><td>4.8ms</td></tr><tr><td>248kb</td><td>32kb</td><td>77</td><td>1977</td><td>129.6ms</td><td>5.0ms</td></tr><tr><td>295kb</td><td>34.4kb</td><td>70</td><td>1722</td><td>141.1ms</td><td>5.8ms</td></tr></tbody></table>

**我们其实只要关注各项优化前后的 QPS 变化即可。**

##### 2） CPU 利用率

由于 Apache Bench 和 Java 部署在同一台机器. 上，所以 CPU 利用率应该减去 Apache Bench  
的 CPU 资源消耗。根据观察，优化前 Apache Bench 的 CPU 消耗在 1.7% 到 2% 之间，优化后  
Apache Bench 的 CPU 资源消耗在 20% 左右。为什么优化前后有这么大的差距呢? 因为优化后  
响应能够及时返回，所以导致 Apache Bench 使用的 CPU 资源多了。

在接下来的计算中，我们将优化前的 CPU 利用率设置为 98%，优化后的 CPU 利用率设置为 80%。

##### 3）CPU Time 计算公式

根据 QPS 的计算方法，把 QPS 挪到右边的分母中，CPU Time 移到等号左边，就会得到下图的公式。

![](https://summer-blog-images.oss-cn-shanghai.aliyuncs.com/qps_optimization/image_004.png)

##### 4) CPU Time 计算示例

根据上方列出的三点（CPU 利用率、QPS 和 CPU 核数），接下里我们就详细的描述一下推算方法了。

#### 计算得到的 CPU Time

根据上方的表格计算方法，利用 QPS 计算出各页面理论上的 CPU Time，计算后的结果如下表：

<table><thead><tr><th>原始页面 92kb</th><th>计算公式</th></tr></thead><tbody><tr><td>优化前 CPU Time 计算</td><td>1000 / 164 x 2 x 0.98 = 12ms</td></tr><tr><td>优化后 CPU Time 计算</td><td>1000 / 2024 x 2 x 0.8 = 0.8ms</td></tr></tbody></table>

<table><thead><tr><th>原始页面大小</th><th>压缩后的大小</th><th>优化前 QPS</th><th>优化后 QPS</th><th>优化前 CPU Time</th><th>优化后 CPU Time</th></tr></thead><tbody><tr><td>92kb</td><td>17kb</td><td>164</td><td>2024</td><td>12ms</td><td>0.8ms</td></tr><tr><td>138kb</td><td>8.7kb</td><td>143</td><td>1859</td><td>13.7ms</td><td>0.86ms</td></tr><tr><td>182kb</td><td>11.4kb</td><td>121</td><td>2083</td><td>16.2ms</td><td>0.77ms</td></tr><tr><td>248kb</td><td>32kb</td><td>77</td><td>1977</td><td>25.5ms</td><td>0.81ms</td></tr><tr><td>295kb</td><td>34.4kb</td><td>70</td><td>1722</td><td>28ms</td><td>0.93ms</td></tr></tbody></table>

这里主要看一下各项的 CPU Time 优化前后的变化，接下来，我们把两个值做减法，然后和开篇中提到过的实测程序中 Gzip 的 CPU Time 进行对比。

#### 实测 CPU Time

##### 1） 5 个页面的 Gzip 所消耗的 CPU Time

实测 5 个页面做 Gzip 所消耗的时间，然后跟公式计算出来的 CPU Time 做一个对比，如下表：

<table><thead><tr><th>原始页面大小</th><th>CPU Time 公式差值 (上表的 CPU Time 差值)</th><th>Gzip CPU Time 测量值 (10 次平均值)</th><th>差值</th></tr></thead><tbody><tr><td>92kb</td><td>11.2ms</td><td>8ms</td><td>3.2ms</td></tr><tr><td>138kb</td><td>12.8ms</td><td>7ms</td><td>5.8ms</td></tr><tr><td>182kb</td><td>15.4ms</td><td>9ms</td><td>6.4ms</td></tr><tr><td>248kb</td><td>24.7ms</td><td>21ms</td><td>3.0ms</td></tr><tr><td>295kb</td><td>27.1ms</td><td>23ms</td><td>4.1ms</td></tr></tbody></table>

可以看到，计算出来的 CPU Time 值要比测出来的要多一点，多了几毫秒，这是为什么呢?  
其实是因为在优化前，有两个消耗 CPU Time 的阶段，一个是执行 Java 代码时，另一个是执行 Gzip 时。而优化后，整个逻辑变成了从缓存获取数据后直接返回，只有非常少的 Java 代码在消耗 CPU Time(10 行以内)。

##### 2） Java 页面执行消耗的 CPU Time

大体上可以认为：

优化前的 CPU Time - 优化后的 CPU Time = Gzip CPU Time + 全页面 Java 代码的 CPU Time

在实验中，一开始只统计了 Gzip 本身的消耗，而在 Java 文件中 Java 代码执行的时间并没有包含在内，所以两者差距比较大。于是，我们单独统计了 5 个页面的 Java 代码的执行时间，发现文件中 Java 码执行的时间为 3~6ms。实际测量的 Gzip CPU Time 加上 3~6ms 的 Java 代码执行时间，和使用公式计算出来的 CPUTime 基本吻合。

根据上面的计算和测量结果，我们发现 Gzip 的 CPU Time 消耗加上 Java 代码的 CPU Time 消耗，与公式测量出来的总的 CPU Time 消耗非常接近，误差为 1~2ms。考虑到 CPU Time 测量是单线程测量，而压力测试 QPS 是并发情况下 (会多出进程切换的开销和 GC 等的开销)，我们认为这点误差是合理的，测试结果说明公式在宏观上是正确的。

### 压力测试最佳线程数和 QPS 临界点

前面讲到了公式的推导，并在一个固定的条件下验证了公式在该场景下的正确性。

假设在一个 thread-per-client 的场景，有一一个 Ajax 请求，这个请求返回一个 Json 字符串，每个请求的 CPU Time 为 1ms，WaitTime 为 300ms(比如读写 Socket 和线程调度的等待开销)。那么最佳线程数是

$$(300+1/1 )x4x 100%=1204$$

尤其在广域网上，Wait Time=300ms 是正常的数值。在国际环境下，300ms 就更加常见了。这意味着如果是 4 核的机器，需要 1204 个线程，如果是 8 核的机器则需要 2408 个线程。实际上，有些 HTTP 服务的 CPU Time 是远小于 1ms 的，比如上面的场景中将页面压缩并缓存起来之后, CPU Time 基本为 0.8ms, 如果 WaitTime 还是 300ms, 那么需要数以千计的线程啊! 当线程数不断增加的时候，到达某个临界点之后对系统就开始产生负面影响了。

(1) 大量线程上下文切换的开销，引起 CPU Time 的增加及 QPS 的减少。所以，有时候还没有达到最佳线程数，而 QPS 已经开始略微下降了。因为 CPU Time 发生变化、线程多了之后，调度引起的 CPU Time 提升的百分比和 QPS 下降的百分比成正比 (上方的 QPS 公式)，上下文切换带来的开销如下。

*   上下文切换 (微妙级别)
*   JVM 本身的开销
*   CPU Cache 加载

(2) 线程的栈空间会占用大量的内存，假设每个线程的栈空间是 1MB，这么多的线程就要占用数 GB 的内存。

(3) 在 CPU Time 不变的情况下，因为线程上下文切换和操作系统想尽力为线程在宏观上平均分配时间片的行为，导致每个线程的 Wait Time 都增加了，于是每个请求的 RT 也增加了，最终就会产生用户体验下降的情况。

可以 用一张图来表示一下临界点的概念。

![](https://summer-blog-images.oss-cn-shanghai.aliyuncs.com/qps_optimization/image_006.png)

由于线程数增加超过某个临界点会影响 CPU Time、QPS 和 RT，所以很难精确测量高并发下的 CPU Time, 它随着机器硬件、操作系统、线程数等因素不断变化。我们能做的就是压力测试 QPS，并在压力测试的过程中调整线程数，使 QPS 达到临界点，这个临界点是 QPS 的一个峰值点。这个峰值点的线程数即当前系统的最佳线程数，当然如果这个时候 CPU 利用率没有达到 100%，那么证明系统中可能存在瓶颈，应该在找到并处理瓶颈之后继续压力测试，并且重新找到这个临界点。

当数据结构发生改变、算法改进或者业务逻辑发生改变时，最佳线程数有可能会跟着变化。

### 总结：

这篇 blog 的例子中，在 CPU Time 下降到 1ms 左右而 Wait Time 需要数百毫秒的场景下，我们需要很多线程。但是当达到这个线程数的时候，有可能早就达到了临界点。所以系统整体已经不是最健康的状态了，但是现有的编程模型已经阻碍了我们前进，那么应该怎么办呢? 为使某个系统达到最优状态?

所以下一篇 blog 我们来说一下编程中的同步模型和异步模型问题，以及为什么异步模型只需要这么少的线程，是不是公式在异步模型下失效了。

我的个人站点  
SUMMER [https://www.huangyingsheng.com/about](https://www.huangyingsheng.com/about)